---
title: "Post gpt"
output: html_notebook
---

```{r set-up, include=F}
knitr::opts_chunk$set(echo = FALSE, warning=F, message=F)
knitr::opts_chunk$set(dev = "png", dev.args = list(type = "cairo-png"))
options(knitr.table.format = "html")
library(tidyverse)
library(jsonlite)
library(here)
library(rlang)
library(lme4)
library(brms)
#library(rstanarm)
library(rstan)
library(viridis)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(theme_bw())

ParseJSONColumn <- function(x) {
  str_c("[ ", str_c(x, collapse = ",", sep=" "), " ]")  %>% 
    fromJSON(flatten = T)
}


```
# New approach

## Take filtered chat sample & pre-do it
```{r}

raw <- read_csv(here("raw_chats/rotate_filtered_chat.csv")) |> filter(numPlayers==4) |> filter(!is.chitchat) |> filter(!is.na(spellchecked)) |>  filter(target=="/experiment/tangram_A.png") |> select(gameId, targetNum, repNum, trialNum, numPlayers, playerId, target, role, countCorrect, spellchecked)
  
raw |> select(text=spellchecked) |> write_csv(here("data/test/sample.csv"))


```

## Post gpt
```{r}

fix_thing <- function(text){
  unlist(strsplit(gsub("[\\[\\]']", "", text, perl = TRUE), ", "))
}

read_data <- function(file){
 read_csv(here(file)) |> mutate(gpt_out = map (gpt_out, .f=fix_thing)) |> unnest(gpt_out) |> mutate(is_substring=str_detect(text, fixed(gpt_out, ignore_case=T )))
}


```

```{r}
post <- read_csv(here("data/test/outsample_gpt3.csv")) |> bind_cols(raw) |> mutate(gpt_out=map(gpt_out, .f=fix_thing)) |> unnest(gpt_out, keep_empty = T) |> 
mutate(is_substring=str_detect(text, fixed(gpt_out, ignore_case=T ))) |> select(text, gpt_out, is_substring, everything())


post_label <- post |> group_by(`...1`, text, role) |> summarize(gpt_out=str_flatten(gpt_out, collapse=" \n\n")) |> write_csv(here("data/test/outsample3_prelabel.csv"))
```

Everything that wasn't a substring was b/c of weirdnesses around punctuation (on both sides) --> might want to run a spellcheck or handcheck on those (8 out of 278 items / 416 descriptions)

```{r}
read_csv(here("data/test/outsample3_labelled.csv")) |> group_by(`Parsing grade`) |> tally()

nrow(post_label)

```



```{r}

nrow(post)
post |> filter(is.na(is_substring)) |> nrow() # 16 blanks, 10 of which *should* have been labelled (and others should have been tagged by spell check) 
post |> filter(!is_substring) |> nrow() # 8 although they weren't bad

```

Ungenerously, 224 / 258 were adequate (87 %) 
Generously,  244 / 258 were adequate (94 %) 

Also missed 10 things that it labelled as blank and shouldn't and 6 were acceptable. So maybe between 84% and 92% acceptable 

Real question is how useful it is down the road



# Number of chunks over time 

* not dealing with 0s in a reasonable way! 

```{r}

post |> group_by(gameId, repNum, role) |> tally() |> ggplot(aes(x=repNum, y=n))+geom_smooth(method="glm")+geom_jitter(width=0.2, height=.2)+facet_grid(.~role)
```
# Length of chunks over time 

* these haven't been cleaned up so this may be pretty unreliable

```{r}

post |> mutate(words=str_count(gpt_out, "\\S+")) |> group_by(gameId, repNum, role) |> ggplot(aes(x=repNum, y=words))+geom_smooth(method="glm")+geom_jitter(width=0.2, height=.2)+facet_grid(.~role)+coord_cartesian(ylim=c(0,15))

post |> mutate(words=str_count(gpt_out, "\\S+")) |> group_by(gameId, repNum, role) |> summarize(words=sum(words)) |> ggplot(aes(x=repNum, y=words))+geom_smooth(method="glm", formula=y~poly(x,3))+geom_jitter(width=0.2, height=.2)+facet_grid(.~role)+coord_cartesian(ylim=c(0,50))
```

looks like both number of chunks and lengths of chunks decrease (although second is not super reliable b/c method needs help). As well as (obvious given the last two) numbers of total words in chunks (which is proxy for referential words) 

# Types of chunks over time 

* will need to figure out a *better* tagging system to get something more sophisticated, but we can work with what we have for now

# Closed class labels

can just label body parts and shapes in a closed class way
```{r}
body_parts <- c("head", "arm", "leg", "foot", "feet", "shoulder", "hand", "back", "face", "neck", "body") # might be problems with back

shapes <- c("triangle", "square", "diamond", "trapezoid", "trapezium", "block", "shape" ) # ? "box"

positional <- c("above", "below", "right", "left", "tilt", "up", "down") # might be problems with the polysemy of right? # ? "level with"

zombie <- c("zombie", "bird", "danc", "thrill", "flag", "karate", "crane", "horse", "hippo", "elephant", "angel", "drunk", "rex", "monster", "workman", "bear") # this is just to approximate what gets used 
# gpt might be better for dealing with polysemy
```

```{r}

closed_class <- post |> mutate(gpt_out=str_to_lower(gpt_out),
                                                 has_body =str_detect(gpt_out, paste(body_parts, collapse = "|")),
                               has_shape = str_detect(gpt_out, paste(shapes, collapse = "|")),
                               has_position = str_detect(gpt_out, paste(positional, collapse = "|")),
                               has_holistic = str_detect(gpt_out, paste(zombie, collapse = "|")))

  
```

Note that these labels are hand-listed with regard to the text used, just trying to get a sense of what we could do here if we figuring out automatic labelling. 

```{r}

closed_class |> group_by(has_body, has_shape, has_position, has_holistic) |> tally() |> arrange(desc(n))

```
common types are: body, body+position, holistic, position, (none), body+shape, 

```{r}

closed_class |> filter(!is.na(gpt_out))|> group_by(has_body, repNum) |> tally() |> ggplot(aes(x=repNum, y=n, color=has_body))+geom_point()+geom_line()+coord_cartesian(ylim=c(0,100))


closed_class |> filter(!is.na(gpt_out))|> group_by(has_position, repNum) |> tally() |> ggplot(aes(x=repNum, y=n, color=has_position))+geom_point()+geom_line()+coord_cartesian(ylim=c(0,100))

closed_class |> filter(!is.na(gpt_out))|> group_by(has_shape, repNum) |> tally() |> ggplot(aes(x=repNum, y=n, color=has_shape))+geom_point()+geom_line()+coord_cartesian(ylim=c(0,100))

closed_class |> filter(!is.na(gpt_out))|> group_by(has_holistic, repNum) |> tally() |> ggplot(aes(x=repNum, y=n, color=has_holistic))+geom_point()+geom_line()+coord_cartesian(ylim=c(0,100))


```
# Try tagging prep 

```{r}

post |> filter(!is.na(gpt_out)) %>% select(gpt_out) |> write_csv(here("data/test/sample_to_tag.csv"))


```

# Tagging check!

```{r}

v_codes <- read_csv(here("data/test/tagged_by_hand.csv")) %>% rename_with( ~str_c(.x,"_v"))

llm_codes <- read_csv(here("data/test/outsample_tag1.csv"))

llm_codes %>% select(body) %>% unique()
llm_codes %>% select(position) %>% unique()
llm_codes %>% select(shape) %>% unique()
llm_codes %>% select(abstract) %>% unique()

both <- v_codes %>% bind_cols(llm_codes) %>% select(-`...7`)

both %>% filter(gpt_out_v !=gpt_out)

agreement <- both %>% mutate(across(c(body, position, shape, abstract), ~ifelse(str_detect(.x, "not"), 0,1))) %>% select(-gpt_out) %>% 
  mutate(body_v_mod=str_c(body_v, "_", body),
         position_v_mod=str_c(position_v, "_", position),
         shape_v_mod=str_c(shape_v, "_", shape),
         abstract_v_mod=str_c(abstract_v, "_", abstract)) 

agreement_table_1 <- agreement %>%   
  select(body_v_mod, position_v_mod, shape_v_mod,abstract_v_mod) %>% 
  pivot_longer(everything()) %>% 
  group_by(name,value) %>% 
  tally() %>% 
  pivot_wider(names_from=value, values_from=n) %>% 
  mutate(pct_match=(`0_0`+`1_1`)/(`0_0`+`0_1`+`1_0`+`1_1`),
         sens=(`1_1`/(`1_1`+`1_0`)),
         spec=(`0_0`)/(`0_1`+`0_0`))
         


```

```{r}

# with examples

v_codes <- read_csv(here("data/test/tagged_by_hand.csv")) %>% rename_with( ~str_c(.x,"_v"))

llm_codes <- read_csv(here("data/test/outsample_examples.csv"))

llm_codes %>% select(body) %>% unique()
llm_codes %>% select(position) %>% unique()
llm_codes %>% select(shape) %>% unique()
llm_codes %>% select(abstract) %>% unique()

both <- v_codes %>% bind_cols(llm_codes) %>% select(-`...7`)

both %>% filter(gpt_out_v !=gpt_out)

agreement <- both %>%
  mutate(part=ifelse(part!="part", "not part", "part")) %>% #remove this line later!
  mutate(across(c(body, position, shape, abstract, part), ~ifelse(str_detect(.x, "not"), 0,1))) %>% select(-gpt_out) %>% 
  mutate(body_v_mod=str_c(body_v, "_", body),
         position_v_mod=str_c(position_v, "_", position),
         shape_v_mod=str_c(shape_v, "_", shape),
         abstract_v_mod=str_c(abstract_v, "_", abstract),
         part_v_mod=str_c(part_v, "_", part)) 

agreement_table <- agreement %>%   
  select(body_v_mod, position_v_mod, shape_v_mod,abstract_v_mod, part_v_mod) %>% 
  pivot_longer(everything()) %>% 
  group_by(name,value) %>% 
  tally() %>% 
  pivot_wider(names_from=value, values_from=n, values_fill=0 ) %>% 
  mutate(pct_match=(`0_0`+`1_1`)/(`0_0`+`0_1`+`1_0`+`1_1`),
         sens=(`1_1`/(`1_1`+`1_0`)),
         spec=(`0_0`)/(`0_1`+`0_0`))


```

TODO How to deal with when it doesn't return a proper answer???


Sensitivity is extremely bad for abstract! Kinda bad for everything.

Okay now sensitivity is now still bad, but less bad for abstract
body -- sensitivity went up, spec went way down :(

position -- just got purely worse :(

shape -- sensitivity got better, spec went down a lot for overall worse

part -- pretty good actually

# TODO notes

May want to clean up gpt-isms like replacing ' with / *sigh*

What tag classes to use?

Get a human-human ceiling of agreement!

What's right metric for chunking agreement?

Isaac/Izzy recommend: 
Charf?

Or loke bleu

But yes one of these weighted ngram overlap metrics that @Isaac Caswell is an expert in
10m

Isaac Caswell
Yeah chrf is the way to go

https://github.com/mjpost/sacrebleu

IC

use like `sacrebleu --i=model_output_file.txt --metric=chrf correct_output_file.txt `



