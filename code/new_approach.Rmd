---
title: "Post gpt"
output: html_notebook
---

```{r set-up, include=F}
knitr::opts_chunk$set(echo = FALSE, warning=F, message=F)
knitr::opts_chunk$set(dev = "png", dev.args = list(type = "cairo-png"))
options(knitr.table.format = "html")
library(tidyverse)
library(jsonlite)
library(here)
library(rlang)
library(lme4)
library(brms)
#library(rstanarm)
library(rstan)
library(viridis)
library(testthat)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(theme_bw())

ParseJSONColumn <- function(x) {
  str_c("[ ", str_c(x, collapse = ",", sep=" "), " ]")  %>% 
    fromJSON(flatten = T)
}

pre_chunk_loc <- "data/test/pre_chunk"

post_chunk_loc <- "data/test/post_chunk"
pre_sub_loc <- "data/test/pre_sub"

post_sub_loc <- "data/test/post_sub"
pre_fix_loc <- "data/test/pre_fix_chunk"

post_fix_loc <- "data/test/post_fix_chunk"
pre_abstract_loc <- "data/test/pre_abstract"

post_abstract_loc <- "data/test/post_abstract"
pre_tag_loc <- "data/test/pre_tag"

post_tag_loc <- "data/test/post_tag"
done_loc <- "data/test/done"

# I think the theory here is that R can write into pre_ files but not post_ files
```


# Pre-chunking

TODO for spellchecked column, remove double spaces before running for future

```{r}

raw_all <- read_csv(here("raw_chats/rotate_filtered_chat.csv")) |> filter(numPlayers==4) |> filter(!is.chitchat) |> filter(!is.na(spellchecked)) |>  select(gameId, targetNum, repNum, trialNum, numPlayers, playerId, target, role, countCorrect, spellchecked) |> mutate(row_id=row_number())

raw_all |> select(text=spellchecked) |> write_csv(here(pre_chunk_loc,"sample_all_4p.csv"))
```
# Run chunk script

# Post chunk stuff
```{r}

fix_thing <- function(text){
  unlist(strsplit(gsub("[\\[\\]']", "", text, perl = TRUE), ", "))
}

read_data <- function(file, raw){
 dat <- read_csv(here(post_chunk_loc,file)) |> 
    bind_cols(raw)
 test_that("file and raw don't line up",{expect_equal(dat$text,dat$spellchecked)})
 
 dat <- dat |> 
    mutate(gpt_out = map (gpt_out, .f=fix_thing)) |> 
    unnest(gpt_out, keep_empty = T) |> 
    mutate(text=str_replace_all(text, "  ", " ")) |> mutate(is_substring=str_detect(text, fixed(gpt_out, ignore_case=T ))) 
 
 dat |> select(text, chunk=gpt_out, row_id,
               gameId, targetNum, trialNum, numPlayers,
               playerId, target, role, is_substring) |> write_csv(here(pre_sub_loc,file))
}

check_substrings <- function(dat){
dat |> filter(!is_substring) |> View()
}

verify_substrings <- function(loc,file){
  d <- read_csv(here(loc,file)) |> 
    mutate(is_substring=str_detect(text, fixed(chunk, ignore_case=T ))) |> filter(!is.na(chunk))
 # test_that("only substrings",{expect_in(d$is_substring, c(T))})
  d |> filter(!is_substring)
  
}

do_string <- function(a,b){
  str_replace(a,coll(b),"*")
}

#TODO add a row_id somewhere that matches utterance and then use that to keep things in order 
# it's the summarize that messes with the order!
diff_text_chunks <- function(file){
  df <- read_csv(here(post_sub_loc,file)) |> 
    select(-is_substring) |> 
    group_by(row_id, across(c(-chunk))) |> 
    mutate(chunk=str_to_lower(chunk)) |> 
    summarize(chunks=list(chunk)) |> 
    mutate(text=str_to_lower(text)) |> 
    rowwise() |> 
    mutate(remainder=reduce(chunks, do_string, .init=text)) |> 
    unnest(chunks, keep_empty=T) |> 
    group_by(across(c(-chunks))) |>
    summarize(chunks=str_c(chunks,collapse="  /*/  ")) |> 
    select(remainder,chunks,text,everything()) |> 
    write_csv(here(pre_fix_loc,file))
}

check_na_rows <- function(dat){
  dat |> filter(is.na(chunk)) |> View()
}

prep_abstract <- function(file){
  d <- read_csv(here(post_fix_loc, file)) |> 
  mutate(chunk=str_split(chunks,coll("/*/"))) |> 
  select(-chunks, -remainder) |> 
  unnest(chunk, keep_empty=T) |> 
  mutate(chunk=trimws(chunk) |> str_replace_all("  ", " "),
         abstract="",
         abstract_2="",
         chunk_id=row_number()) |> 
    filter(!is.na(chunk)) |> 
  select(abstract_2, abstract,chunk_id,chunk, everything()) |> 
  write_csv(here(pre_abstract_loc,file))
}

prep_tag <- function(file){
  read_csv(here(post_abstract_loc,file)) |> 
  filter(!is.na(chunk)) |> 
  select(-abstract, -abstract_2) |> 
    mutate(abstract= ifelse(is.na(abstract_final),0,abstract_final)) |> 
  arrange(chunk_id) |> 
  select(-text, -abstract_final) |> 
  write_csv(here(pre_tag_loc, file))
}

# do_post_tag <- function(file){
#   read_csv(here(post_tag_loc,file)) |> 
#   mutate(is_abstract=abstract,
#          is_body=ifelse(str_detect(body,"not"),0,1),
#          is_position=ifelse(str_detect(position,"not"),0,1),
#          is_shape=ifelse(str_detect(shape,"not"), 0,1),
#          is_posture=ifelse(str_detect(posture,"not"),0,1)) |> 
#   select(-`...1`, -abstract, -body, -position, -shape, -posture) |> 
#     write_csv(here(done_loc,file))
# }

detect_body <- str_c("\\b(face|head|heads|back|shoulder|shoulders",
                 "|arm|arms|leg|legs|foot|feet|body|knee|knees|",
                 "toe|toes|hand|hands|body|butt|heel|heels|ear|ears|nose|neck|chest|hair)\\b")

detect_shape <- str_c("squar|triangle|triangular|diamond|shape|",
"trapez|angle|degree|parallel|rhomb|box|cube")

detect_position <- str_c("right|left|above|below|under|over|top|bottom|behind|side|beneath")

detect_posture <- str_c("kick|crouch|squat|kneel|knelt|stood|",
                        "stand|sit|sat|lying|walk|facing|fall|looking|",
"lean|seat|laying")


do_tag <- function(file) {
  read_csv(here(pre_tag_loc,file)) |>   mutate(regex_body=ifelse(str_detect(chunk,detect_body),1,0),
         regex_shape=ifelse(str_detect(chunk, detect_shape),1,0),
         regex_position=ifelse(str_detect(chunk, detect_position),1,0),
         regex_posture=ifelse(str_detect(chunk, detect_posture),1,0)) |> write_csv(here(done_loc, file))}

```

```{r}
testing <- read_data("4p_all.csv", 
                     raw_all)

```

## check things that weren't substrings
should get resolved into a copy of the pre_abstract file

```{r}
#check_substrings(testing)
```

## verify only substrings

```{r}
#verify_substrings(post_sub_loc,"4p_all.csv") |> View()
```
## find what wasn't used

```{r}
# look for lines where nothing was used
testing <- diff_text_chunks("4p_all.csv")
```

## prep for abstract

TODO how to do substring verification where it'll show the lines that error!

TODO did we lose the NAs somewhere??

```{r}
test <- prep_abstract("4p_all.csv")

verify_substrings(pre_abstract_loc,"4p_all.csv")

#test |>  mutate(is_substring=str_detect(text, fixed(chunk, ignore_case=T ))) |> filter(!is.na(chunk)) |> filter(!is_substring)


```

## pre tag 

```{r}


 test <- prep_tag("4p_all.csv")
```

## go run tagging here

or actually maybe the gpt tagging is pretty iffy and we should just doing it closed class?

## post tag

what do we do about the tagging sometimes being bad?

```{r}
do_tag("4p_all.csv")
```

# pre-sbert

```{r}
 a <- read_csv(here(done_loc,"4p_all.csv")) # this on chunk will just work for sbert 

a |> group_by(gameId,targetNum, trialNum, numPlayers, playerId, target, role) |> 
  summarize(across(starts_with("is"),~list(.)),
            sentence=str_c(chunk, collapse=", ")) |> write_csv(here(done_loc,"4p_all_concat.csv"))


```

# post-sbert

```{r}
library(reticulate)
np <- import("numpy")
mat = np$load(here("data/test/post_sbert/4p_all.npy"))
saveRDS(mat,here("data/test/post_sbert/4p_all.RData"))
mat = np$load(here("data/test/post_sbert/4p_all_concat.npy"))
saveRDS(mat,here("data/test/post_sbert/4p_all_concat.RData"))

```
# Old stuff
## Chunking agreement TODO
<!--
```{r}
V <- read_csv(here("data/test/outsample_V.csv")) |> 
  mutate(rownum=row_number()) |> 
  mutate(`text...2`=str_replace_all(`text...2`,"//", "//**//")) |> 
  select(rownum, text=`text...1`, chunks=`text...2`) 

llm <- read_csv(here("data/test/outsample_gpt3.csv")) |> mutate(gpt_out=map(gpt_out, .f=fix_thing)) |> unnest(gpt_out, keep_empty = T) |> 
  group_by(`...1`, text) |> 
  summarize(chunks=str_c(gpt_out, collapse=" //**// ")) |> 
  ungroup() |> 
  select(rownum=`...1`, text, chunks)

test_that("same length", {expect_equal(nrow(V),nrow(llm))
  #expect_equal(V$text, llm$text)
  #this fails because quotes are a nightmare
  })

#let's try to figure out what format charf wants

V |> select(chunks) |> write_csv(here("data/test/pre_charf_correct.txt"), col_names=F)

llm |> select(chunks) |> write_csv(here("data/test/pre_charf_model.txt"), col_names=F)
```

https://github.com/mjpost/sacrebleu

run `sacrebleu --i=pre_charf_model.txt --metric=chrf --chrf-word-order 2  --chrf-whitespace  pre_charf_correct.txt`

not sure what settings actually make sense -->



